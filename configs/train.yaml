defaults:
  - config  # Inherit from base config.yaml

# Hydra settings
hydra:
  # Use default Hydra output directory (outputs/)
  # To customize: uncomment and set dir: ${paths.output_dir}
  # run:
  #   dir: ${paths.output_dir}
  job:
    chdir: false

# Training Hyperparameters (Self-Forcing specific)
training:
  pretrained_checkpoint: ./outputs/2026-01-17/10-46-03/checkpoint_step_039000.pt    # Pretrained checkpoint to load
  num_steps: 100000              # Number of training steps
  batch_size: 4               # Batch size
  lr: 1e-3                   # Learning rate (1e-3)
  weight_decay: 0.01           # Weight decay for optimizer
  gradient_clip_norm: 1.0      # Gradient clipping max norm
  dfake_gen_update_ratio: 4   # Update generator every N steps (DMD2-style)

  # Self-Forcing video generation
  num_frames: 21               # Number of frames for training (4 blocks Ã— 3 frames)
  num_frames_per_block: 3      # Frames per block
  denoising_steps: [1000, 750, 500, 250]  # Denoising timesteps
  context_noise: 0             # Noise level for cache update
  
  # DMD-specific configs (optional, with defaults)
  num_train_timestep: 1000      # Number of training timesteps
  guidance_scale: 1.0            # Guidance scale for generation
  timestep_shift: 1.0            # Timestep shift factor
  ts_schedule: false             # Use timestep schedule
  ts_schedule_max: false         # Maximum timestep schedule
  min_score_timestep: 0          # Minimum score timestep
  min_num_frames: 21             # Minimum number of frames
  max_num_frames: null           # Maximum number of frames (null = use training_num_frames)

  # Training control
  save_interval: 1000         # Save checkpoint every N steps
  log_interval: 10             # Log metrics every N steps
  viz_interval: 100            # Generate sample videos every N steps

  # Dataset
  num_samples: 20              # Number of training samples in dataset
  video_frames: 9              # Number of frames per video in dataset



# Checkpoint to load (null for training from scratch)
# For fine-tuning after pretraining:
#   python train.py checkpoint=logs/pretrain/checkpoint_final.pt
checkpoint: null

# Wandb settings (separate project for pretraining)
wandb:
  enabled: true
  project: "minimal-self-forcing"
  entity: dpose-team
  name: null
